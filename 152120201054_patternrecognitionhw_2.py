# -*- coding: utf-8 -*-
"""152120201054_PatternRecognitionHW_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RJP8vLCzHG2-jIUzUaQBBViuOV2jVxKU
"""

# Pattern Recognition - HW#2
import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import cifar10

# Step 1: Load the CIFAR-10 dataset
print("Loading CIFAR-10 dataset...")
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Display dataset information
print(f"Training data shape: {x_train.shape}")
print(f"Training labels shape: {y_train.shape}")
print(f"Test data shape: {x_test.shape}")
print(f"Test labels shape: {y_test.shape}")

# Step 2: Display some sample images
plt.figure(figsize=(10, 4))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(x_train[i])
    plt.title(f"Class: {y_train[i][0]}")
    plt.axis('off')
plt.tight_layout()
plt.show()

# Define class names for CIFAR-10
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

# Step 3: Convert images to vector format as specified in the assignment
print("Converting images to vectors...")
x_train = x_train.reshape(-1, 3072)
x_test = x_test.reshape(-1, 3072)

# Normalize the data to [0,1] range
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Step 4: Define custom similarity function exactly as specified in the assignment
def custom_similarity(x, y):
    """
    Compute custom similarity between vectors x and y using the formula:
    similarity = (exp(A) * exp(B)) / (exp(A) + exp(B) - A*B)
    where A is x (test sample) and B is y (train sample).
    """
    # Ensure inputs are numpy arrays with correct shape
    if len(x.shape) == 1:
        x = x.reshape(1, -1)
    if len(y.shape) == 1:
        y = y.reshape(1, -1)

    # Calculate A and B directly (the vectors themselves)
    A = x
    B = y

    # Calculate dot product for A*B term
    A_dot_B = np.sum(A * B)

    # Calculate norms for exp(A) and exp(B)
    norm_A = np.linalg.norm(A)
    norm_B = np.linalg.norm(B)

    # Compute similarity using the given formula
    numerator = np.exp(norm_A) * np.exp(norm_B)
    denominator = np.exp(norm_A) + np.exp(norm_B) - A_dot_B

    # Avoid division by zero
    if denominator == 0:
        return 0

    similarity = numerator / denominator
    return similarity

# Test the custom similarity function with the example from the assignment
print("\nTesting custom similarity function with example from assignment:")
x = np.array([-0.2, 0])  # test sample
y = np.array([1, 1])     # train sample
x = x.reshape(1, -1)
y = y.reshape(1, -1)
s = custom_similarity(x, y)
print('similarity:', s)

# Step 5: Implement KNN classifier with custom similarity
def knnCustomSimilarity(x_train, y_train, sample_test, k):
    """
    KNN classifier using custom similarity metric.

    Parameters:
    x_train: Training data features
    y_train: Training data labels
    sample_test: Test sample to classify
    k: Number of nearest neighbors to consider

    Returns:
    Class name of the predicted class
    """
    n_samples = x_train.shape[0]
    similarities = np.zeros(n_samples)

    # Calculate similarity between test sample and all training samples
    for i in range(n_samples):
        similarities[i] = custom_similarity(sample_test, x_train[i])

    # Get indices of k most similar samples (sorted in descending order)
    most_similar_indices = np.argsort(-similarities)[:k]

    # Get the labels of the k most similar samples
    most_similar_labels = y_train[most_similar_indices].flatten()

    # Count occurrences of each label
    unique_labels, counts = np.unique(most_similar_labels, return_counts=True)

    # Find the label with the most occurrences
    predicted_label = unique_labels[np.argmax(counts)]

    # Return the class name
    return class_names[predicted_label]

# Step 6: Test the KNN classifier with a sample as specified in the assignment
print("\nTesting KNN classifier with custom similarity...")
sample_index = 0  # Using the first test sample as in the assignment example
sample_test = x_test[sample_index, :]
k = 5

# Show sample test
plt.figure(figsize=(3, 3))
plt.imshow(x_test[sample_index].reshape(32, 32, 3))
plt.title(f"Test Sample (index={sample_index})")
plt.axis('off')
plt.show()

# Test the classifier with the sample
similar_class_name = knnCustomSimilarity(x_train, y_train, sample_test, k)
print(f"For test sample index {sample_index} with k={k}:")
print(f"Predicted class name: {similar_class_name}")
print(f"True class name: {class_names[y_test[sample_index][0]]}")

# Try with another example from the assignment
print("\nTrying with another test sample (index=1) as mentioned in the assignment:")
sample_test = x_test[1, :]
similar_class_name = knnCustomSimilarity(x_train, y_train, sample_test, k)
print(f"For test sample index 1 with k={k}:")
print(f"Predicted class name: {similar_class_name}")
print(f"True class name: {class_names[y_test[1][0]]}")

# Step 7: Evaluate the model on multiple test samples
def evaluate_knn(x_train, y_train, x_test, y_test, k, num_samples=100):
    """
    Evaluate KNN classifier on multiple test samples.

    Parameters:
    x_train, y_train: Training data
    x_test, y_test: Test data
    k: Number of neighbors
    num_samples: Number of test samples to evaluate (default: 100)

    Returns:
    Accuracy of the classifier
    """
    correct = 0

    # Limit evaluation to specified number of samples
    n_eval = min(num_samples, x_test.shape[0])

    for i in range(n_eval):
        sample_test = x_test[i, :]
        true_label = y_test[i][0]

        # Get predicted class name
        predicted_class_name = knnCustomSimilarity(x_train, y_train, sample_test, k)
        predicted_label = class_names.index(predicted_class_name)

        if predicted_label == true_label:
            correct += 1

    accuracy = correct / n_eval
    return accuracy

# Test with different k values
k_values = [1, 3, 5, 7, 9]
num_eval_samples = 100  # Limit for faster execution

print("\nEvaluating KNN performance with different k values...")
print(f"Using {num_eval_samples} test samples for evaluation")

for k in k_values:
    accuracy = evaluate_knn(x_train, y_train, x_test, y_test, k, num_eval_samples)
    print(f"Accuracy with k={k}: {accuracy:.4f}")

# Optionally: Plot confusion matrix for best k value
try:
    from sklearn.metrics import confusion_matrix
    import seaborn as sns

    def plot_confusion_matrix(x_train, y_train, x_test, y_test, k, num_samples=100):
        """
        Plot confusion matrix for KNN classifier.
        """
        # Limit evaluation to specified number of samples
        n_eval = min(num_samples, x_test.shape[0])

        # Initialize predictions and true labels
        predictions = []
        true_labels = []

        for i in range(n_eval):
            sample_test = x_test[i, :]
            true_label = y_test[i][0]
            true_labels.append(true_label)

            # Get predicted class name
            predicted_class_name = knnCustomSimilarity(x_train, y_train, sample_test, k)
            predicted_label = class_names.index(predicted_class_name)
            predictions.append(predicted_label)

        # Create confusion matrix
        cm = confusion_matrix(true_labels, predictions)

        # Plot confusion matrix
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=class_names, yticklabels=class_names)
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.title(f'Confusion Matrix (k={k})')
        plt.show()

    # Plot confusion matrix for best k value
    best_k = 5  # Assuming k=5 based on evaluation
    print(f"\nPlotting confusion matrix for k={best_k}...")
    plot_confusion_matrix(x_train, y_train, x_test, y_test, best_k, num_samples=100)
except ImportError:
    print("\nSkipping confusion matrix (requires sklearn and seaborn)")

print("\nAssignment completed!")